#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¯æ—¥è‡ªå‹•ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
launchd ã‹ã‚‰æ¯æ—¥00:00ã«å®Ÿè¡Œã•ã‚Œã‚‹
"""

import sqlite3
import json
import os
import sys
from datetime import datetime
from pathlib import Path

def backup_database():
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’JSONãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã§ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—"""
    
    # ãƒ‘ã‚¹è¨­å®š
    base_dir = Path(__file__).parent
    db_path = base_dir / 'notion.db'
    backup_dir = base_dir / 'backups'
    
    if not db_path.exists():
        print(f"âŒ Database not found: {db_path}")
        return False
    
    # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    backup_dir.mkdir(exist_ok=True)
    
    try:
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š
        conn = sqlite3.connect(str(db_path))
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§å–å¾—ï¼ˆFTSãƒ†ãƒ¼ãƒ–ãƒ«ã‚’é™¤å¤–ï¼‰
        cursor.execute("""
            SELECT name FROM sqlite_master 
            WHERE type='table' 
            AND name NOT LIKE 'sqlite_%'
            AND name NOT LIKE '%_fts%'
            AND name NOT LIKE '%_config'
            ORDER BY name
        """)
        tables = [row[0] for row in cursor.fetchall()]
        
        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿
        backup_data = {
            'timestamp': datetime.now().isoformat(),
            'database': 'notion.db',
            'tables': {}
        }
        
        # å„ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
        for table in tables:
            cursor.execute(f'SELECT * FROM {table}')
            rows = cursor.fetchall()
            backup_data['tables'][table] = [dict(row) for row in rows]
        
        conn.close()
        
        # JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_file = backup_dir / f'backup_{timestamp}.json'
        
        with open(backup_file, 'w', encoding='utf-8') as f:
            json.dump(backup_data, f, ensure_ascii=False, indent=2)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º
        file_size = backup_file.stat().st_size / 1024
        
        # æœ€æ–°ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ latest.json ã«ã‚³ãƒ”ãƒ¼
        latest_file = backup_dir / 'latest.json'
        with open(latest_file, 'w', encoding='utf-8') as f:
            json.dump(backup_data, f, ensure_ascii=False, indent=2)
        
        # ãƒ­ã‚°å‡ºåŠ›ï¼ˆlaunchdçµŒç”±ã§å®Ÿè¡Œã•ã‚Œã‚‹ãŸã‚ã€ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã«è¨˜éŒ²ï¼‰
        log_file = base_dir / 'backups' / 'daily_backup.log'
        with open(log_file, 'a', encoding='utf-8') as log:
            log.write(f"[{datetime.now().isoformat()}] âœ… Backup created: {backup_file.name} ({file_size:.1f}KB)\n")
        
        # å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’å‰Šé™¤ï¼ˆæœ€æ–°30å€‹ã‚’ä¿æŒï¼‰
        cleanup_old_backups(backup_dir, max_backups=30)
        
        print(f"âœ… Backup created: {backup_file.name}")
        return True
        
    except Exception as e:
        print(f"âŒ Backup failed: {e}")
        # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚‚è¨˜éŒ²
        log_file = base_dir / 'backups' / 'daily_backup.log'
        with open(log_file, 'a', encoding='utf-8') as log:
            log.write(f"[{datetime.now().isoformat()}] âŒ Backup failed: {e}\n")
        return False

def cleanup_old_backups(backup_dir, max_backups=30):
    """å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’å‰Šé™¤ï¼ˆæœ€æ–°Nå€‹ã‚’ä¿æŒï¼‰"""
    try:
        backup_files = sorted(backup_dir.glob('backup_*.json'), reverse=True)
        
        # æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«ã‚ˆã‚Šå¤ã„ã‚‚ã®ã‚’å‰Šé™¤
        for old_backup in backup_files[max_backups:]:
            old_backup.unlink()
            log_file = backup_dir / 'daily_backup.log'
            with open(log_file, 'a', encoding='utf-8') as log:
                log.write(f"[{datetime.now().isoformat()}]   ğŸ—‘ï¸ Deleted old backup: {old_backup.name}\n")
    except Exception as e:
        print(f"âš ï¸ Cleanup failed: {e}")

if __name__ == '__main__':
    success = backup_database()
    sys.exit(0 if success else 1)
